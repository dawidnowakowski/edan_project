{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0218ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import nltk\n",
    "import tqdm\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f371bf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hubert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Hubert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hubert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Hubert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99c53361",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05acc3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"<.*?>\", \"\", text) # remove html tags like <br /> etc\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text) # remove URLs\n",
    "    text = re.sub(r\"[^a-z\\s]\", \"\", text) # remove punctuation (!?- etc) and numbers\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip() # remove unnecesary whitespaces (fe \"  \")\n",
    "    return text\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize_text_nltk(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatized = [\n",
    "        lemmatizer.lemmatize(token, get_wordnet_pos(tag))\n",
    "        for token, tag in tagged_tokens\n",
    "    ]\n",
    "    return \" \".join(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3322cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [04:28<00:00, 185.95it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewer have mention that af...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production the filming tech...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i think this be a wonderful way to spend time ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically theres a family where a little boy j...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter matteis love in the time of money be a ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>i think this movie do a down right good job it...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>bad plot bad dialogue bad act idiotic direct t...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>i be a catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>im go to have to disagree with the previous co...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>no one expect the star trek movie to be high a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  label\n",
       "0      one of the other reviewer have mention that af...  positive      1\n",
       "1      a wonderful little production the filming tech...  positive      1\n",
       "2      i think this be a wonderful way to spend time ...  positive      1\n",
       "3      basically theres a family where a little boy j...  negative      0\n",
       "4      petter matteis love in the time of money be a ...  positive      1\n",
       "...                                                  ...       ...    ...\n",
       "49995  i think this movie do a down right good job it...  positive      1\n",
       "49996  bad plot bad dialogue bad act idiotic direct t...  negative      0\n",
       "49997  i be a catholic taught in parochial elementary...  negative      0\n",
       "49998  im go to have to disagree with the previous co...  negative      0\n",
       "49999  no one expect the star trek movie to be high a...  negative      0\n",
       "\n",
       "[50000 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"] = df[\"review\"].apply(clean_text)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"sentiment\"])  # positive=1, negative=0\n",
    "df_lemmatized=df.copy(deep=True)\n",
    "df_lemmatized[\"review\"] = df_lemmatized[\"review\"].progress_apply(lemmatize_text_nltk)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"review\"], df[\"label\"], test_size=0.2, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "X_lemmatize_train, X_lemmatize_test, y_lemmatize_train, y_lemmatize_test = train_test_split(\n",
    "    df_lemmatized[\"review\"], df_lemmatized[\"label\"], test_size=0.2, stratify=df_lemmatized[\"label\"]\n",
    ")\n",
    "\n",
    "df_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe41b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=\"english\")),\n",
    "    (\"clf\", LinearSVC())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__max_features\": [10000, 20000],\n",
    "    \"tfidf__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"tfidf__min_df\": [1, 5],\n",
    "    \"tfidf__max_df\": [0.95, 0.9],\n",
    "    \"clf__C\": [0.1, 1, 10],\n",
    "}\n",
    "# Najlepsze parametry: {'clf__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 2)}\n",
    "# Najlepszy wynik CV bez lematyzacji: 0.8885500411096597\n",
    "\n",
    "# Najlepsze parametry: {'clf__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2)}\n",
    "# Najlepszy wynik CV z lematyzacją: 0.8869499354853003\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__max_features\": [5000, 10000, 20000],\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1,3)],\n",
    "    \"tfidf__min_df\": [1,2],\n",
    "    \"tfidf__max_df\": [0.95,0.9],\n",
    "    \"tfidf__sublinear_tf\": [True, False],\n",
    "    \"clf__C\": [0.1, 1],\n",
    "}\n",
    "# Najlepsze parametry: {'clf__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 3), 'tfidf__sublinear_tf': True}\n",
    "# Najlepszy wynik bez lematyzacji CV: 0.8905000148627534\n",
    "\n",
    "# Najlepsze parametry: {'clf__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 3), 'tfidf__sublinear_tf': True}\n",
    "# Najlepszy wynik CV z lematyzacją: 0.8885248998631595"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73da412a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "Najlepsze parametry: {'clf__C': 0.1, 'tfidf__max_df': 0.95, 'tfidf__max_features': 20000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 3), 'tfidf__sublinear_tf': True}\n",
      "Najlepszy wynik CV: 0.8885248998631595\n"
     ]
    }
   ],
   "source": [
    "grid = GridSearchCV(\n",
    "    pipeline,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "grid.fit(X_lemmatize_train, y_lemmatize_train)\n",
    "\n",
    "print(\"Najlepsze parametry:\", grid.best_params_)\n",
    "print(\"Najlepszy wynik CV:\", grid.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
